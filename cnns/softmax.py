import numpy as np

class Softmax:
    def __init__(self,input_len,nodes):
        self.weights = np.random.randn(input_len,nodes)
        self.biases = np.zeros(nodes)

    def forward(self,input):
        self.last_input_shape = input.shape
        input = input.flatten()
        self.last_input = input
        totals = np.dot(input,self.weights)+self.biases
        self.last_totals = totals
        exp=np.exp(totals)
        return exp/np.sum(exp,axis=0)

    def backprop(self,output,label,learn_rate):
        self.d_L_d_out = np.zeros(10)#1x10
        self.d_L_d_out[label] = -1/output[label]

        S=np.sum(np.exp(self.last_totals),axis=0)#1x1
        d_S_d_t=np.exp(self.last_totals)#1x10
        d_outc_d_t=-np.exp(self.last_totals[label])*d_S_d_t/(S**2)#1x10
        d_outc_d_t[label]+=np.exp(self.last_totals[label])/S
        d_t_d_w =self.last_input
        d_t_d_b = 1
        d_t_d_inputs =self.weights

        d_L_d_t = self.d_L_d_out[label]*d_outc_d_t#1x10
        d_L_d_b = d_L_d_t * d_t_d_b#1x10
        d_L_d_w =d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]
        d_L_d_inputs =d_t_d_inputs @ d_L_d_t#这里d_L_d_t是（10，）并非1x10的矩阵，在这里可以等价为10x1的矩阵

        self.weights -= learn_rate * d_L_d_w
        self.biases -= learn_rate * d_L_d_b
        return d_L_d_inputs.reshape(self.last_input_shape)

    # def backprop(self, d_L_d_out, learn_rate):
    #     '''
    #     Performs a backward pass of the softmax layer.
    #     Returns the loss gradient for this layer's inputs.
    #     - d_L_d_out is the loss gradient for this layer's outputs.
    #     - learn_rate is a float    '''
    #     # We know only 1 element of d_L_d_out will be nonzero
    #     for i, gradient in enumerate(d_L_d_out):
    #         if gradient == 0:
    #             continue
    #
    #         # e^totals
    #         t_exp = np.exp(self.last_totals)
    #
    #         # Sum of all e^totals
    #         S = np.sum(t_exp)
    #
    #         # Gradients of out[i] against totals
    #         d_out_d_t = -t_exp[i] * t_exp / (S ** 2)
    #         d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)
    #
    #         # Gradients of totals against weights/biases/input
    #         d_t_d_w = self.last_input
    #         d_t_d_b = 1
    #         d_t_d_inputs = self.weights
    #
    #         # Gradients of loss against totals
    #         d_L_d_t = gradient * d_out_d_t
    #
    #         # Gradients of loss against weights/biases/input
    #         d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]
    #         d_L_d_b = d_L_d_t * d_t_d_b
    #         d_L_d_inputs = d_t_d_inputs @ d_L_d_t
    #
    #         # Update weights / biases
    #         self.weights -= learn_rate * d_L_d_w
    #         self.biases -= learn_rate * d_L_d_b
    #         return d_L_d_inputs.reshape(self.last_input_shape)